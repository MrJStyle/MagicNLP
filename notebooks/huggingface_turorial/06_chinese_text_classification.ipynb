{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset chn_senti_corp (/Users/luominzhi/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a106eb3c4284793b496bc259518a8b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(path=\"seamew/ChnSentiCorp\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载字典和分词工具"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tokenized_ds = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=dataset[\"text\"],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=250,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"tf\",\n",
    "    return_length=True,\n",
    "    # return_token_type_ids=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 ， 有 电 动 扶 梯 直 接 到 达 海 边 ， 周 围 餐 馆 、 食 廊 、 商 场 、 超 市 、 摊 位 一 应 俱 全 。 酒 店 装 修 一 般 ， 但 还 算 整 洁 。 泳 池 在 大 堂 的 屋 顶 ， 因 此 很 小 ， 不 过 女 儿 倒 是 喜 欢 。 包 的 早 餐 是 西 式 的 ， 还 算 丰 富 。 服 务 吗 ， 一 般 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[\"input_ids\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(20, 250), dtype=int32, numpy=\narray([[ 101, 6848, 2885, ...,    0,    0,    0],\n       [ 101, 8115,  119, ...,    0,    0,    0],\n       [ 101, 2791, 7313, ...,    0,    0,    0],\n       ...,\n       [ 101, 2523, 1962, ...,    0,    0,    0],\n       [ 101, 4023,  778, ...,    0,    0,    0],\n       [ 101, 1079, 2100, ...,    0,    0,    0]], dtype=int32)>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[\"input_ids\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(20,), dtype=int32, numpy=\narray([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n      dtype=int32)>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(dataset[\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建训练 dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luominzhi/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/cache-bd19e88c8423571b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1200 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c952e9ee038044d2a4a54f1c910ccb4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1200 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9ea8a6983104c1bac83b43081fd14b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = dataset.map(\n",
    "    lambda example: tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=200)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/luominzhi/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85/cache-e1da0efa7158ae11.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_ds = tokenized_ds[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_ds = tokenized_ds[\"test\"].shuffle(seed=42).select(range(1000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 1000\n})"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "tf_train_ds = small_train_ds.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\",],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_test_ds = small_test_ds.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\",],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "({'input_ids': <tf.Tensor: shape=(8, 200), dtype=int64, numpy=\n  array([[ 101, 1355, 4178, ...,    0,    0,    0],\n         [ 101, 6843, 4638, ...,    0,    0,    0],\n         [ 101, 6821, 3315, ...,    0,    0,    0],\n         ...,\n         [ 101,  702,  782, ...,    0,    0,    0],\n         [ 101, 6841, 1217, ...,    0,    0,    0],\n         [ 101, 1762, 6821, ...,    0,    0,    0]])>,\n  'token_type_ids': <tf.Tensor: shape=(8, 200), dtype=int64, numpy=\n  array([[0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]])>,\n  'attention_mask': <tf.Tensor: shape=(8, 200), dtype=int64, numpy=\n  array([[1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         ...,\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0]])>},\n <tf.Tensor: shape=(8,), dtype=int64, numpy=array([0, 0, 0, 1, 1, 1, 1, 1])>)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tf_train_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载预训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "pretrained = TFBertModel.from_pretrained(\"bert-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  102267648 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,267,648\n",
      "Trainable params: 102,267,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型试算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(8, 200, 768), dtype=float32, numpy=\n",
      "array([[[ 1.1308393 ,  0.2345363 , -0.8766137 , ..., -0.26060915,\n",
      "         -0.04310881, -0.58737534],\n",
      "        [-0.1569882 ,  0.56958866,  0.6289089 , ..., -0.768068  ,\n",
      "         -0.11376459,  0.35377085],\n",
      "        [ 0.78638947,  0.5999495 ,  0.64621556, ...,  0.2696762 ,\n",
      "          1.5119927 , -0.22727573],\n",
      "        ...,\n",
      "        [-0.52723414, -0.24690893, -0.53745407, ...,  0.35378206,\n",
      "          0.8578799 , -0.93970907],\n",
      "        [-0.43473202,  0.46251753, -0.1544686 , ...,  0.4311419 ,\n",
      "          0.5793288 , -1.2472293 ],\n",
      "        [ 0.54778236,  0.44366223, -0.22687349, ...,  0.06052589,\n",
      "          0.08866881, -0.4830872 ]],\n",
      "\n",
      "       [[-0.20123546, -0.10884465,  0.2623894 , ..., -0.3264447 ,\n",
      "          0.29584184, -0.13884443],\n",
      "        [-0.01765029,  0.5939071 , -0.41618973, ..., -0.9018427 ,\n",
      "         -0.16438168,  0.02866795],\n",
      "        [ 1.4683472 ,  0.08501804, -1.3277123 , ...,  1.0056981 ,\n",
      "          0.1095897 , -0.44253358],\n",
      "        ...,\n",
      "        [ 0.52605754,  0.62056416,  0.36898205, ...,  0.5673394 ,\n",
      "         -0.02803729, -0.80933225],\n",
      "        [ 0.2921328 ,  0.10484791,  0.25701588, ...,  0.6590259 ,\n",
      "          0.19286287, -0.9348223 ],\n",
      "        [ 0.42952728,  0.86689615,  0.51132244, ...,  0.74003345,\n",
      "          0.01392075, -0.8095549 ]],\n",
      "\n",
      "       [[ 0.14878532,  0.1846132 ,  0.20537002, ..., -0.78469014,\n",
      "          0.05106391, -0.26831433],\n",
      "        [ 0.06334418,  0.47750485,  0.57566786, ..., -0.86202806,\n",
      "         -0.45206165,  0.15841064],\n",
      "        [ 1.2393337 , -0.09285698, -0.16905756, ...,  1.3519235 ,\n",
      "          1.097324  , -0.3968597 ],\n",
      "        ...,\n",
      "        [ 0.24915427, -0.16050631, -0.564242  , ...,  0.06540816,\n",
      "          0.05565879, -0.38760635],\n",
      "        [ 0.427744  , -0.11705114, -0.46227586, ..., -0.19790137,\n",
      "          0.31283224, -0.46566698],\n",
      "        [ 0.45179603, -0.20852551, -0.48299164, ..., -0.1857341 ,\n",
      "          0.30728814, -0.51541084]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.02075334,  0.40082994, -0.09393829, ...,  0.5626992 ,\n",
      "         -0.30354735, -0.28110558],\n",
      "        [ 0.55291206,  0.13067427,  0.07338607, ..., -0.53293264,\n",
      "         -0.65412515, -0.10080095],\n",
      "        [ 0.1355372 ,  0.74995923, -0.691507  , ...,  0.6912116 ,\n",
      "          0.0432521 ,  0.2694964 ],\n",
      "        ...,\n",
      "        [ 0.16443059, -0.21828002, -0.70773417, ...,  0.99276066,\n",
      "         -0.01178685, -0.04332335],\n",
      "        [-0.10661348,  0.03134479, -0.29015818, ...,  0.52254146,\n",
      "          0.11773157, -0.28826883],\n",
      "        [ 0.01425641, -0.30115712, -0.5853541 , ...,  0.78251487,\n",
      "          0.22395778, -0.16495709]],\n",
      "\n",
      "       [[ 0.1899784 , -0.19781911, -0.3092975 , ..., -0.27636665,\n",
      "          0.39385724, -0.6727438 ],\n",
      "        [ 0.6700908 ,  0.2347573 ,  1.0588748 , ..., -1.1068151 ,\n",
      "          0.08466028,  0.18594149],\n",
      "        [-0.13832648, -0.5578691 , -0.28712758, ...,  0.92737305,\n",
      "          1.4329946 ,  0.28151459],\n",
      "        ...,\n",
      "        [ 0.5454779 , -0.70391953, -0.8907449 , ...,  0.22260134,\n",
      "         -0.05677156, -0.39476666],\n",
      "        [ 0.43857098, -0.64705807, -0.5605405 , ..., -0.01405194,\n",
      "         -0.20955434, -0.43532848],\n",
      "        [ 0.48694757, -0.6752505 , -0.66019905, ...,  0.02518064,\n",
      "         -0.18947363, -0.50725704]],\n",
      "\n",
      "       [[ 0.8966843 ,  0.52363   ,  0.12766813, ...,  0.29506993,\n",
      "          1.0421245 , -0.51832885],\n",
      "        [-0.25228137,  0.11207439,  0.87477535, ..., -1.1963221 ,\n",
      "         -0.45168626,  0.12005027],\n",
      "        [-0.66252434, -1.1239468 ,  0.2819225 , ..., -0.25457805,\n",
      "          0.8765674 , -0.24745207],\n",
      "        ...,\n",
      "        [ 0.3124556 ,  0.14274958, -0.23788475, ...,  0.12171295,\n",
      "         -0.09343151, -0.70707554],\n",
      "        [ 0.22697909,  0.24502385, -0.12491283, ...,  0.19653803,\n",
      "         -0.1416057 , -0.71698207],\n",
      "        [ 0.15120924,  0.3024289 , -0.25204432, ...,  0.18676625,\n",
      "         -0.011393  , -0.797416  ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(8, 768), dtype=float32, numpy=\n",
      "array([[ 0.99951714,  0.98077756,  0.99937195, ..., -0.9779013 ,\n",
      "        -0.99995035, -0.36551827],\n",
      "       [ 0.9998341 ,  0.999688  ,  0.99909365, ..., -0.9536555 ,\n",
      "        -0.999831  ,  0.4933308 ],\n",
      "       [ 0.9999185 ,  0.9998277 ,  0.9998347 , ..., -0.98261434,\n",
      "        -0.9999361 ,  0.7150343 ],\n",
      "       ...,\n",
      "       [ 0.99972343,  0.9998749 ,  0.99943095, ..., -0.98393005,\n",
      "        -0.9990418 ,  0.6944751 ],\n",
      "       [ 0.9992887 ,  0.9999045 ,  0.99489474, ..., -0.86873776,\n",
      "        -0.99990034,  0.7867977 ],\n",
      "       [ 0.9998288 ,  0.99978316,  0.9998872 , ..., -0.9793249 ,\n",
      "        -0.99990034, -0.43451974]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "single_batch = next(iter(tf_train_ds))\n",
    "\n",
    "out = pretrained(\n",
    "    input_ids=single_batch[0][\"input_ids\"],\n",
    "    attention_mask=single_batch[0][\"attention_mask\"],\n",
    "    token_type_ids=single_batch[0][\"token_type_ids\"],\n",
    ")\n",
    "\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([8, 200, 768])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.4 ms ± 290 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pretrained(input_ids=single_batch[0][\"input_ids\"], attention_mask=single_batch[0][\"attention_mask\"], token_type_ids=single_batch[0][\"token_type_ids\"],)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义下游任务模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Model, self).__init__(*args, **kwargs)\n",
    "        self.fc = tf.keras.layers.Dense(2)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        x = pretrained(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        x = self.fc(x.pooler_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [95]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/python-3.9/lib/python3.9/site-packages/keras/engine/training.py:2869\u001B[0m, in \u001B[0;36mModel.summary\u001B[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001B[0m\n\u001B[1;32m   2847\u001B[0m \u001B[38;5;124;03m\"\"\"Prints a string summary of the network.\u001B[39;00m\n\u001B[1;32m   2848\u001B[0m \n\u001B[1;32m   2849\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2866\u001B[0m \u001B[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001B[39;00m\n\u001B[1;32m   2867\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2868\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilt:\n\u001B[0;32m-> 2869\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2870\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis model has not yet been built. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   2871\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBuild the model first by calling `build()` or by calling \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   2872\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe model on a batch of data.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   2873\u001B[0m layer_utils\u001B[38;5;241m.\u001B[39mprint_summary(\n\u001B[1;32m   2874\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   2875\u001B[0m     line_length\u001B[38;5;241m=\u001B[39mline_length,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2878\u001B[0m     expand_nested\u001B[38;5;241m=\u001B[39mexpand_nested,\n\u001B[1;32m   2879\u001B[0m     show_trainable\u001B[38;5;241m=\u001B[39mshow_trainable)\n",
      "\u001B[0;31mValueError\u001B[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.build()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([0, 1, 0, 1, 0, 1, 1, 0])>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(model(next(iter(tf_train_ds))[0]), axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 20:05:27.920914: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.6620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 20:05:59.607706: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 61s 426ms/step - loss: 0.6219 - accuracy: 0.6620 - val_loss: 0.4502 - val_accuracy: 0.8360\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 42s 334ms/step - loss: 0.4815 - accuracy: 0.7740 - val_loss: 0.4082 - val_accuracy: 0.8380\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 42s 336ms/step - loss: 0.4625 - accuracy: 0.7940 - val_loss: 0.3963 - val_accuracy: 0.8350\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2f98a4100>"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(tf_train_ds, validation_data=tf_test_ds, epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}